{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cEboxXNNHXly"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "device = \"cuda\"\n",
        "\n",
        "random_seed = 6\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ],
      "metadata": {
        "id": "N17oLGkpHftI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./MNIST/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./MNIST/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "wIp68p_WHlVt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 10)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "        self.cf = nn.Linear(10, 50)\n",
        "\n",
        "    def forward(self, x, pred_prob=None):\n",
        "        x = self.activation(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.activation(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.activation(self.fc1(x))\n",
        "\n",
        "        if pred_prob is not None:\n",
        "            pred_prob = torch.nn.functional.softmax(pred_prob, 1)\n",
        "            x += self.activation(self.cf(pred_prob))\n",
        "\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "t3CXPKqeHlzW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net().to(device)\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Av8yDtCaHwE1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ],
      "metadata": {
        "id": "b9TfMhZ3HydI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_train(x, N = 3):\n",
        "    preds = []\n",
        "    pred = None\n",
        "    for _ in range(N):\n",
        "        pred = network(x, pred)\n",
        "        preds.append(pred)\n",
        "    return torch.cat(preds)\n",
        "\n",
        "def inference_test(x, N = 3):\n",
        "    preds = []\n",
        "    pred = None\n",
        "    for _ in range(N):\n",
        "        pred = torch.nn.functional.softmax(network(x, pred), 1)\n",
        "        preds.append(pred)\n",
        "    return sum(preds) / N"
      ],
      "metadata": {
        "id": "xdh7CeqpH8Rs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = inference_train(data)\n",
        "\n",
        "    loss = F.nll_loss(output, target.tile(3))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "#       torch.save(network.state_dict(), '/vanilla_model.pth')\n",
        "#       torch.save(optimizer.state_dict(), '/vanilla_optimizer.pth')\n",
        "\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = inference_test(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "l8J7gph4IBK-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQwPqCUtIELo",
        "outputId": "3362a9e3-1215-4105-8bc7-5e04247f2ae5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-bcdc46fa1a32>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: -0.0985, Accuracy: 984/10000 (10%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.334754\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.720626\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.457343\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.351678\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.275641\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.132344\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.344049\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.164020\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.123514\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.147569\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.355349\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.158286\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.412770\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.272593\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.112904\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.102868\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.409741\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.048710\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.290640\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.115131\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.033946\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.044022\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.136540\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.039413\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.080181\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.215904\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.062141\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.027225\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.120103\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.208108\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.224936\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.103477\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.182138\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.164553\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.173832\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.237070\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.152805\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.040732\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.008562\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.084780\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.167801\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.107572\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.213437\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.121620\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.106242\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.148022\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.239765\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.164735\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.204399\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.035430\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.212083\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.077730\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.129992\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.118936\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.024485\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.176115\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.261485\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.023169\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.271440\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.304132\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.187976\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.159664\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.147753\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.082592\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.012653\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.068315\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.198376\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.263581\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.212623\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.087726\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.284763\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.220883\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.111245\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.065101\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.086622\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.129309\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.196228\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.072642\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.085055\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.295318\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.084453\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.248623\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.046359\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.177651\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.008133\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.094260\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.293224\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.126907\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.166106\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.428436\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148259\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.256179\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.181842\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.052670\n",
            "\n",
            "Test set: Avg. loss: -0.9608, Accuracy: 9713/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.269898\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.087436\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.039652\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.060659\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.057468\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.236409\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.047596\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.199791\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.220381\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.116463\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.101853\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.217419\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.041719\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.044472\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.125052\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.021113\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.077412\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.407544\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.093036\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.114580\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.320781\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.075761\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.143833\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.057177\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.038031\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.023112\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.086843\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.155047\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.426019\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.053744\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.248991\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.152498\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.133427\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.029728\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.131064\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.056615\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.117090\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.046582\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.213168\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.834331\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.223341\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.103011\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.034072\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.023075\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.056232\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.117702\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.090613\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.179467\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.196827\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.087969\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.184280\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.125385\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.069773\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.037284\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.054320\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.219905\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.084884\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.141246\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.012306\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.028734\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.052073\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.042097\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.075055\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.145630\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.172553\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.059345\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.106519\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.107798\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.145745\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.101984\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.129985\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.317626\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.248981\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.242574\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.109180\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.070863\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.129414\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.023904\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.060935\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.229847\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.068230\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.103527\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.001781\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.073641\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.198976\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.128538\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.014913\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.030092\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.132396\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.124830\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.030487\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.130914\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.070950\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.030501\n",
            "\n",
            "Test set: Avg. loss: -0.9696, Accuracy: 9745/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.025153\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.197792\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.094760\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.183798\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.010546\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.025726\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.024604\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.049651\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.006791\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.099222\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.049674\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.133232\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.037922\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.231775\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.036755\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.358555\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.254688\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.316307\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.113955\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.286524\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.194173\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.086836\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.007787\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.043937\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.012698\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.008728\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.456544\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.100650\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.099334\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.049411\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.071547\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.022747\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.006226\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.284855\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.099434\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.024391\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.011459\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.004717\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.085723\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.117402\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.144446\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.070764\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.138569\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.017706\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.013331\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.007793\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.006277\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.007414\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.025478\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.070877\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.196259\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.038106\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.180798\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.033311\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.268792\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.000998\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.192484\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.111881\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.029392\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.027311\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.019655\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.155062\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.188101\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.045225\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.000438\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.004313\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.164642\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.096379\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.146634\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.093248\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.713283\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.145687\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.152073\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.202963\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.086133\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.075560\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.034901\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.070311\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.129358\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.006071\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.275823\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.244090\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.116766\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.147888\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.642778\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.024280\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.260976\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.003462\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.059370\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.027010\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.050757\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.075356\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.094423\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.197668\n",
            "\n",
            "Test set: Avg. loss: -0.9667, Accuracy: 9736/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = 1e-3\n",
        "\n",
        "for _ in range(2):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR2duiTZIGkY",
        "outputId": "b940f8e7-501e-41eb-e98a-1a41ee5b8286"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-bcdc46fa1a32>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.000492\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.010211\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.000408\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.009567\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.001071\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.031634\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.039150\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.076876\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.000230\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.001069\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.036519\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.013639\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.033857\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.183817\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.014725\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.005941\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.001451\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.006575\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.000261\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.144304\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.017009\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.073872\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.007024\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.086244\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001755\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.103732\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.005390\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.001339\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001713\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.002226\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.053898\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.005260\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.044980\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.000295\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.031326\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.021600\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.080282\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.010896\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.097234\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.000125\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.003074\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.038005\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.048819\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.025705\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.012594\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.061642\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.013424\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.072235\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.018695\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.019304\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.039707\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.108107\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.022680\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.033368\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.038425\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.035315\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.003674\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.063560\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.000693\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.003752\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.030448\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.006005\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.016710\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.313230\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.019492\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.002056\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.002833\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.007300\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.013340\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.032789\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.013025\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.002673\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.025567\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.045351\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.004705\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.001575\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.073311\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.004023\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.001466\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.080683\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.000957\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.029754\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.010776\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.035869\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.002310\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.013785\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.016405\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.089402\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.001284\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.000532\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.085558\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.029401\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.078213\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.000558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: -0.9867, Accuracy: 9891/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_fashion_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.FashionMNIST('./FMNIST/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_fashion_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.FashionMNIST('./FMNIST/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "LfO5qfyIISEk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing OOD metrics\n",
        "\n",
        "network.eval()\n",
        "\n",
        "uncertainties = np.array([])\n",
        "labels = np.array([])\n",
        "eps = 1e-10\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "\n",
        "    data = data.to(device)\n",
        "    prob = inference_test(data)\n",
        "    uncertainty = (-prob * torch.log(prob + eps)).sum(dim=1).cpu().detach().numpy()\n",
        "    label = np.zeros_like(uncertainty)\n",
        "\n",
        "    uncertainties = np.concatenate([uncertainties, uncertainty])\n",
        "    labels = np.concatenate([labels, label])\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data, target in test_fashion_loader:\n",
        "\n",
        "    data = data.to(device)\n",
        "\n",
        "    prob = inference_test(data)\n",
        "    uncertainty = (-prob * torch.log(prob + eps)).sum(dim=1).cpu().detach().numpy()\n",
        "    label = np.ones_like(uncertainty)\n",
        "\n",
        "    uncertainties = np.concatenate([uncertainties, uncertainty])\n",
        "    labels = np.concatenate([labels, label])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siZj8y_8IVDg",
        "outputId": "f912f135-655c-4e81-ff0b-0d3eb4a4c456"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-bcdc46fa1a32>:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "roc_auc = sklearn.metrics.roc_auc_score(labels, uncertainties)\n",
        "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, uncertainties)\n",
        "pr_auc = sklearn.metrics.auc(recall, precision)\n",
        "roc_auc, pr_auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUdYoVzdIYZW",
        "outputId": "3ff72c83-41d1-4c73-d9b1-5fd20220065d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.974304505, 0.9800322945563024)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lxy3IUwzMK_x"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}